{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image as I\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(pd.read_csv('Datasets/train.csv'))\n",
    "print(data.shape)\n",
    "data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:,0]\n",
    "x = data[:,1:]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = x[:int(0.8*len(x))]\n",
    "ytrain = y[:int(0.8*len(y))]\n",
    "xtest = x[int(0.8*len(x)):]\n",
    "ytest = y[int(0.8*len(y)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data  :  xtrain , ytrain\n",
    "\n",
    "test data   :  xtest  , ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_sz = 4\n",
    "n_il = 784 # no. of input criterias\n",
    "n_fl = 10  # no. of neurons in the hidden layer \n",
    "n_ol = 10  # no. of neurons in the final layer\n",
    "\n",
    "w_nl = 0.1 # weight normaliser\n",
    "b_nl = 0.01 # bias normaliser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Net is defined as :\n",
    "- First layer (input) : takes in 28 pixel * 28 pixel or array of 784 pixel values and writes them as neuron activations\n",
    "\n",
    "- Second layer (hidden) : Condensing the neuron activations of the first layer neurons into 10 neurons of second layer\n",
    " \n",
    "- Third layer (output) : Final layer with 10 neurons, activation of which depicts the probablity of that number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"layers.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = (torch.randperm(len(xtrain)-bat_sz))\n",
    "# randomising the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are the connections that are supposed decided by the model these connect the layers \n",
    "\n",
    "Biases are the additive adjustments made to the layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_23960\\2603008244.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w1 = torch.tensor(torch.randn((n_il,n_fl))*w_nl).clone().detach().requires_grad_(True)   # Weight connecting input layer and layer one\n",
      "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_23960\\2603008244.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b1 = torch.tensor(torch.randn((bat_sz,n_fl))*b_nl).clone().detach().requires_grad_(True)\n",
      "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_23960\\2603008244.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w2 = torch.tensor(torch.randn((n_fl,n_ol)) * w_nl).clone().detach().requires_grad_(True)   # Weight connecting first layer and output layer\n",
      "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_23960\\2603008244.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b2 = torch.tensor(torch.randn((bat_sz,n_ol)) * b_nl).clone().detach().requires_grad_(True)\n"
     ]
    }
   ],
   "source": [
    "w1 = torch.tensor(torch.randn((n_il,n_fl))*w_nl).clone().detach().requires_grad_(True)   # Weight connecting input layer and layer one\n",
    "b1 = torch.tensor(torch.randn((bat_sz,n_fl))*b_nl).clone().detach().requires_grad_(True)\n",
    "w2 = torch.tensor(torch.randn((n_fl,n_ol)) * w_nl).clone().detach().requires_grad_(True)   # Weight connecting first layer and output layer\n",
    "b2 = torch.tensor(torch.randn((bat_sz,n_ol)) * b_nl).clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "parame = [w1,b1,w2,b2]   # parameters to be trained\n",
    "optimizer = torch.optim.SGD(parame, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining forward pass of the neural network (non training)\n",
    "\n",
    "def forward(ex):\n",
    "    z1 = ex.float() @ w1 + b1\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = a1 @ w2 + b2\n",
    "    a2 = torch.softmax(z2, dim=1)\n",
    "    out = a2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "loss :  0.20457471907138824\n",
      "Accuracy :  89.29761904761905\n",
      "epoch : 1\n",
      "loss :  0.11343421041965485\n",
      "Accuracy :  90.76190476190476\n",
      "epoch : 2\n",
      "loss :  0.09013481438159943\n",
      "Accuracy :  91.22619047619047\n",
      "epoch : 3\n",
      "loss :  0.08057047426700592\n",
      "Accuracy :  91.76190476190477\n",
      "epoch : 4\n",
      "loss :  0.07600472122430801\n",
      "Accuracy :  92.0\n",
      "epoch : 5\n",
      "loss :  0.07475019991397858\n",
      "Accuracy :  91.98809523809524\n",
      "epoch : 6\n",
      "loss :  0.07500419020652771\n",
      "Accuracy :  92.07142857142857\n",
      "epoch : 7\n",
      "loss :  0.07525043189525604\n",
      "Accuracy :  91.97619047619048\n",
      "epoch : 8\n",
      "loss :  0.074757419526577\n",
      "Accuracy :  92.19047619047619\n",
      "epoch : 9\n",
      "loss :  0.07333246618509293\n",
      "Accuracy :  92.28571428571428\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    for i in ix:\n",
    "        xt = xtrain[i:i+bat_sz]\n",
    "        yt = ytrain[i:i+bat_sz]\n",
    "        x_in = xt.view([bat_sz,n_il])\n",
    "        y_out = F.one_hot(yt,n_ol).float()\n",
    "        \n",
    "        # ------------------------------------------- forward pass --------------------------------------------------------\n",
    "        z1 = x_in @ w1 + b1   # w1 is multiplied by input layer neurons and b1 bias is added to the output\n",
    "        a1 = torch.sigmoid(z1)   # sigmoid activation\n",
    "        z2 = a1 @ w2 + b2        # n_fl, n_ol\n",
    "        t = z2 \n",
    "\n",
    "        # -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # t is the activations of the output layer (10 activations for all 4 training examples) \n",
    "        loss = F.cross_entropy(t,y_out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # backward paopogation is simply calculating dloss/dw1 , dloss/dw2 , dloss/db1 , dloss/db2 \n",
    "\n",
    "        optimizer.step()\n",
    "        # optimiser it just adjusting the weights with the gradient above calculated\n",
    "        # simply put\n",
    "        # w1 = w1 - l_r*(dloss/dw1)   is the main mathematical function\n",
    "\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"epoch :\", e)\n",
    "    print(\"loss : \", loss.item())\n",
    "    \n",
    "    # Testing accuracy each epoch on the test dataset\n",
    "    k,d=len(xtest),len(xtest)\n",
    "    for n in range(len(xtest)):\n",
    "        omt =list((forward(xtest[n]))[0])\n",
    "        k-=omt.index(max(omt))==ytest[n].item()\n",
    "    print(\"Accuracy : \",((d-k)/d) *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Draw and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  92.28571428571428\n"
     ]
    }
   ],
   "source": [
    "# lets check its accuracy on the inputs it has never encountered\n",
    "k,d=len(xtest),len(xtest)\n",
    "for n in range(len(xtest)):\n",
    "    omt =list((forward(xtest[n]))[0])\n",
    "    k-=omt.index(max(omt))==ytest[n].item()\n",
    "print(\"Accuracy : \",((d-k)/d) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is ur image \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = I.open('checker.png')\n",
    "pixels = im.load()\n",
    "arr = []\n",
    "for k in range(28):\n",
    "    for l in range(28):\n",
    "        arr.append(pixels[l,k][0])\n",
    "tarr = torch.tensor(arr)   #x[204])#\n",
    "parr = tarr.reshape([28,28])\n",
    "print('this is ur image ')\n",
    "plt.gray()\n",
    "plt.imshow(parr, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my prediction is :  2\n"
     ]
    }
   ],
   "source": [
    "oot =list((forward(tarr))[0])\n",
    "print('my prediction is : ',oot.index(max(oot)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
