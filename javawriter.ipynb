{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2i5lL2MYVG96"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This training would be done on the coding language Java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ALrN3NUVyei"
      },
      "outputs": [],
      "source": [
        "with open('Datasets/text/javaer.txt', 'r') as f:\n",
        "    data = f.read()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CuOVdD2XV81i"
      },
      "outputs": [],
      "source": [
        "# now we have the data\n",
        "vocab = sorted(list(set(data)))\n",
        "voc_sz = len(vocab)  #--\n",
        "data_sz = len(data)  #--\n",
        "\n",
        "itoa = {}\n",
        "atoi = {}\n",
        "for i,ele in enumerate(vocab):\n",
        "    atoi[ele] = i\n",
        "    itoa[i] = ele"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "80Tis-hKV_1j"
      },
      "outputs": [],
      "source": [
        "train = np.zeros([data_sz,1])\n",
        "output = np.zeros([data_sz,1])\n",
        "for i in range(data_sz):\n",
        "    train[i] = atoi[data[i]]\n",
        "    if i!=0:\n",
        "        output[i-1] = atoi[data[i]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WotO8-tUWBlz",
        "outputId": "da4365c1-ac5e-4718-f434-ef985b264faa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4000000, 1)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An RNN layer is a repeating neural net architecture. Basically it cycles through a set of input tokens (lets say an example of 8 token sequence) while keeping the previous token's activation (memory kind of) continued to the next token.\n",
        "\n",
        "Maintaining the activations of the previous token helps to enhance some aspects of the upcoming tokens and continue to establish the activation mapping of the entire sequence\n",
        "\n",
        "It can be thought of a compressed mapping of a sequence "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iMg17EceWFKE"
      },
      "outputs": [],
      "source": [
        "# lets define an RNN layer\n",
        "\n",
        "class layer:\n",
        "    def __init__(self,Wax:torch.tensor,Waa:torch.tensor,Wy:torch.tensor,ba:torch.tensor):\n",
        "        self.Wax = Wax\n",
        "        self.Waa = Waa\n",
        "        self.Wy = Wy\n",
        "        self.ba = ba\n",
        "        self = self\n",
        "        pass\n",
        "\n",
        "    def forward(self,A:torch.tensor,X:torch.tensor):\n",
        "\n",
        "        X_inp = torch.tensor((X,voc_sz))\n",
        "        A_act = torch.tanh((self.Wax @ X_inp) + self.Waa @ A + self.ba)\n",
        "        Y_act = (self.Wy @ A_act)\n",
        "        logits = torch.tanh(Y_act)\n",
        "        logits.requires_grad = True\n",
        "        return A_act,logits\n",
        "\n",
        "    def optimise(self,logit:torch.tensor,Y:torch.tensor):\n",
        "        y = torch.tensor(Y)\n",
        "        loss = F.cross_entropy(logit,y)\n",
        "        opt = torch.optim.SGD(params=[self.Wax,self.Waa,self.Wy,self.ba,self.by],lr=0.1)\n",
        "        #opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gL2cGNw5WI16"
      },
      "outputs": [],
      "source": [
        "# -------------------------------- definitions ---------------------------------\n",
        "n_hid = 64\n",
        "epochs = 3\n",
        "batch = 200\n",
        "iteration = int(data_sz/batch)\n",
        "\n",
        "Wax = torch.rand(n_hid, voc_sz) *0.01\n",
        "Waa = torch.rand(n_hid, n_hid) *0.01\n",
        "Wy = torch.rand(voc_sz, n_hid)\n",
        "# ba = torch.zeros(n_hid,1)\n",
        "\n",
        "Wax.requires_grad = True\n",
        "Waa.requires_grad = True\n",
        "Wy.requires_grad = True\n",
        "# ba.requires_grad = True\n",
        "\n",
        "parameters = [Wax,Waa,Wy]\n",
        "# opt = torch.optim.SGD([Wax,Waa,Wy],lr=0.1)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "bteICaMv0d_L"
      },
      "outputs": [],
      "source": [
        "lr = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(parameters, lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The activations are reset every new sequence (Training example) but the weights and biases are the matrices that are ultimately trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "D-c3oj6RWLVy",
        "outputId": "23fadcbe-8d05-4eff-fec6-96037122635f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_13272\\3347931127.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_inp = torch.tensor(F.one_hot(X_input,voc_sz)).view([voc_sz,1])\n",
            "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_13272\\3347931127.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits = F.softmax(Y_act.view([voc_sz])).view([1,voc_sz])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss:  4.543487548828125\n",
            "loss:  4.543487548828125\n",
            "loss:  4.525536060333252\n",
            "loss:  4.543487548828125\n",
            "loss:  4.525536060333252\n"
          ]
        }
      ],
      "source": [
        "# learning loop\n",
        "#A_new.requires_grad = True\n",
        "\n",
        "# ___ The traaining should be done much more than 10 ___\n",
        "for x in range(5): # data_sz-batch):\n",
        "    ix = random.randint(0, data_sz)\n",
        "    A_old  = torch.zeros(n_hid,1)\n",
        "    OLP = Wax.clone().detach()\n",
        "    #print(Wax.grad)\n",
        "    #opt.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "    for i in range(batch):\n",
        "        \n",
        "        # ---------------------------- inpt/outp -------------------------------\n",
        "        X_input = torch.tensor(train[ix+i]).view([1,1]).long()\n",
        "        X_inp = torch.tensor(F.one_hot(X_input,voc_sz)).view([voc_sz,1])\n",
        "        Y = torch.tensor(output[ix+i]).long()\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "        # ----------------------------- forward --------------------------------\n",
        "        A_new = torch.tanh((Wax @ X_inp.float()) + Waa @ A_old)*0.3\n",
        "        Y_act = torch.relu(Wy @ A_new)\n",
        "        logits = F.softmax(Y_act.view([voc_sz])).view([1,voc_sz])\n",
        "        probs = torch.exp(logits)/torch.sum(torch.exp(logits[0]))\n",
        "        # ----------------------------------------------------------------------\n",
        "        # print(probs)\n",
        "\n",
        "        # ----------------------------- Backprop -------------------------------\n",
        "        loss = F.cross_entropy(probs,Y)\n",
        "        loss.backward()\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "        #-------- Memory maintain -------\n",
        "        A_old = A_new.clone().detach()\n",
        "        #--------------------------------\n",
        "    optimizer.step()\n",
        "    # Wax -= lr*Wax.grad   \n",
        "    # Waa -= lr*Waa.grad  \n",
        "    # Wy -= lr*Wy.grad\n",
        "    print(\"loss: \",loss.item())\n",
        "    \n",
        "\n",
        "    # break\n",
        "    #print(Waa ,Waa.grad)\n",
        "    # print(Wax,\"\\n\",Waa,\"\\n\",Wy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "/\\ This is not the entire training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we say optimization this is what is happening under the hood :\n",
        " \n",
        "Wax -= (lr) x (Wax.grad)   \n",
        "Waa -= (lr) x (Waa.grad)  \n",
        "Wy -= (lr) x (Wy.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ukzjGLlq0FnG"
      },
      "outputs": [],
      "source": [
        "def forward(arg:str):\n",
        "    X_input = torch.tensor(atoi[arg]).view([1,1]).long()\n",
        "    X_inp = torch.tensor(F.one_hot(X_input,voc_sz)).view([voc_sz,1])\n",
        "    A_pe = torch.tanh((Wax @ X_inp.float()) + Waa @ A_new)\n",
        "    Y_act = (Wy @ A_pe)\n",
        "    logits = torch.tanh(Y_act)\n",
        "    probs = F.softmax(logits.view([voc_sz]))\n",
        "    return probs\n",
        "def predict(arg:str,lent:int):\n",
        "    output = \"\"\n",
        "    for i in range(lent):\n",
        "        probs = forward(arg)\n",
        "        ix = torch.multinomial(probs, num_samples=1).item()\n",
        "        output+=itoa[ix]\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "K1_l3YT91FmX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "\n",
            "l>|,sD f9mk+\\l:i,w=ZO\t&Rwc^Q@./bz8u\t/#_\n",
            ".+\n",
            "*lf[}|}Xbm\n",
            ">V9L1>9r|_\"y$lluljMl5pC+C|\n",
            "<Q3YT>\t9|aN{Q-Zgx S\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_13272\\876156986.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_inp = torch.tensor(F.one_hot(X_input,voc_sz)).view([voc_sz,1])\n",
            "C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_13272\\876156986.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = F.softmax(logits.view([voc_sz]))\n"
          ]
        }
      ],
      "source": [
        "arr = predict(\" \",100)\n",
        "print(\"-------------------------------------------\\n\")\n",
        "print(arr)\n",
        "print(\"\\n--------------------------------------------\")\n",
        "print(\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is not an ideal output or training it was just to signify how an RNN works \n",
        "I hope this helps"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
