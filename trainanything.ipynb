{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('datasets/train.txt','r',encoding=\"utf8\")\n",
    "total_str = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all the unique tokens (in this case characters) from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(list(total_str))))\n",
    "voc_sz = len(vocab)\n",
    "print(''.join(vocab))  # printing all the unique character\n",
    "print(voc_sz)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the encoding and decoding dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "itoa = {}  # index to token \n",
    "atoi = {}  # token to index\n",
    "for i,ele in enumerate(vocab):\n",
    "    atoi[ele] = i+1\n",
    "    itoa[i] = ele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the string to an index torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata = []\n",
    "for e in total_str:\n",
    "    Xdata.append(atoi[e])\n",
    "Xdata = torch.tensor(Xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntxt = 8   # context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = []\n",
    "Yt = []\n",
    "for i in range(len(Xdata)-cntxt):\n",
    "    Xt.append(Xdata[i:i+cntxt])\n",
    "    Yt.append(Xdata[i+cntxt])\n",
    "Xt = torch.stack(Xt)\n",
    "Yt = torch.stack(Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1597676"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  1, 30, 39, 42, 29, 47, 39]), tensor(42))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt[0],Yt[0]  # dataset fist look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = Xt[:int(0.8*len(Xt))]\n",
    "ytrain = Yt[:int(0.8*len(Yt))]\n",
    "xval = Xt[int(0.8*len(Xt)):int(0.9*len(Xt))]\n",
    "yval = Yt[int(0.8*len(Yt)):int(0.9*len(Yt))]\n",
    "xtest = Xt[int(0.9*len(Xt)):]\n",
    "ytest = Yt[int(0.9*len(Yt)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntxt = 8  # context length (rewritten)\n",
    "n_fl = 100  # number of neurons in 1st layer\n",
    "n_ml = 256  # number of neurons in 2nd layer\n",
    "n_octms = voc_sz  # output layer \n",
    "l_r = 0.0001  # learning rate\n",
    "nl_r = 0.00001 # new learning rate\n",
    "bat_sz = 4  # batch size\n",
    "epoch = 10  # number of iterations on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the creation of the model, the number of layers / activation funstion use / neuron per layer is entirely dependent on the dataset and thus needs to be customised by dryrunning the dataset on equivalent possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment the activation function which you want to try out below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(cntxt,n_fl),  # layer one \n",
    "    #nn.ReLU(),\n",
    "    #nn.Tanh(),   \n",
    "    #nn.Sigmoid(),\n",
    "    nn.Linear(n_fl,n_ml),   # layer two\n",
    "    #nn.ReLU(),\n",
    "    #nn.Tanh(),\n",
    "    #nn.Sigmoid(),\n",
    "    # nn.Dropout(0.0),\n",
    "    nn.Linear(n_ml,n_octms),  # output layer\n",
    "    #nn.ReLU(),\n",
    "    #nn.Tanh(),\n",
    "    #nn.Sigmoid(),\n",
    "    # nn.Softmax(),\n",
    ")\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=l_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomised indices selection for the training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = (torch.randperm(len(xtrain)-bat_sz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy funtion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    k,d=len(xval),len(xval)\n",
    "    for n in range(len(xval)):\n",
    "        omt = ((model(xval[n]))).tolist()\n",
    "        # print(omt.index(max(omt)),ytest[n].item())\n",
    "\n",
    "        k -= (omt.index(max(omt))==yval[n].item())\n",
    "    accu = (((d-k)/d) *100)\n",
    "    return(\"Accuracy = \" + str(accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch of samples at once are sent into the model to calculate the possible output of the token which is then matched with the real value to calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10, Accuracy = 76.64565739200646\n",
      "2 / 10, Accuracy = 79.02380952380952\n",
      "3 / 10, Accuracy = 80.92323845957248\n",
      "4 / 10, Accuracy = 81.82793295923897\n",
      "5 / 10, Accuracy = 82.35965486465846\n",
      "6 / 10, Accuracy = 83.06614656651612\n",
      "7 / 10, Accuracy = 83.81238464879212\n",
      "8 / 10, Accuracy = 84.19588455623352\n",
      "9 / 10, Accuracy = 84.32623587899555\n",
      "10 / 10, Accuracy = 84.45945224522255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    for i in ix:\n",
    "        xt = xtrain[i:i+bat_sz]    # selecting the batch \n",
    "        yt = ytrain[i:i+bat_sz]     \n",
    "        X_train = xt.view([bat_sz,cntxt])   # creating the appropriate tensor size\n",
    "        Y_train = F.one_hot(yt,voc_sz).float()   # creating a one hot vector representation of the possible output\n",
    "\n",
    "        t = model(X_train.float())\n",
    "        loss = F.cross_entropy(t,Y_train)    # Loss definition\n",
    "        optimizer.zero_grad()    \n",
    "\n",
    "        loss.backward()     # accumulating the gradient\n",
    "        optimizer.step()    # rectifying the weights using the gradients\n",
    "        \n",
    "        # print(loss.item())\n",
    "        \n",
    "    print(str(e+1) + \" / \" + str(epoch)+\", \" + accuracy())\n",
    "    if(e == int(0.8*epoch)):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=nl_r)  # decreasing the learning rate \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters (Weights and biases) of the model are rectified in the direction opposite to the slope at the rate of learning specified \n",
    "In other words the weights and biases are getting better each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of traditional neural net testing on neural net, we can expect the accuracy to reach about 84% due to the absence of atention (transformer architecture) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inper = [0]*bat_sz\n",
    "leng = 400\n",
    "for i in range(leng):\n",
    "    oot = (model(torch.tensor(inper[-bat_sz:])))\n",
    "    inper.append(oot.index(max(oot)))\n",
    "    print(itoa[oot.index(max(oot))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to take away from here is setting up a neural net is easier as compared to actually using it for the real world applications. The quality of this neural net is not great that being said it helps you all to understand hot a neural network is applied on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go make the models you want and test out stuff\n",
    "Happy learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
