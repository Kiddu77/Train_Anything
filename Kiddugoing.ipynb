{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Datasets/Text/RT.txt','r',encoding=\"utf8\")\n",
    "total_str = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(list(total_str))))\n",
    "voc_sz = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "print(''.join(vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itoa = {}\n",
    "atoi = {}\n",
    "for i,ele in enumerate(vocab):\n",
    "    atoi[ele] = i\n",
    "    itoa[i] = ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata = []\n",
    "for e in total_str:\n",
    "    Xdata.append(atoi[e])\n",
    "Xdata = torch.tensor(Xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = []\n",
    "Yt = []\n",
    "for i in range(len(Xdata)-8):\n",
    "    Xt.append(Xdata[i:i+8])\n",
    "    Yt.append(Xdata[i+8])\n",
    "Xt = torch.stack(Xt)\n",
    "Yt = torch.stack(Yt)\n",
    "# Yt = F.one_hot(Yt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  1, 30, 39, 42, 29, 47, 39]), tensor(42))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt[0],Yt[0] # nice data arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = Xt[:int(0.9*len(Xt))]/voc_sz\n",
    "ytrain = Yt[:int(0.9*len(Yt))]\n",
    "xtest = Xt[int(0.9*len(Xt)):]/voc_sz\n",
    "ytest = Yt[int(0.9*len(Yt)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables defining : \n",
    "\n",
    "head_sz = 32\n",
    "B,T,C = 4,8,32  # attention but yeah\n",
    "n_fl = 100\n",
    "n_ml = 256\n",
    "# n_ol = 32\n",
    "n_octms = len(vocab)\n",
    "l_r = 0.0001\n",
    "ex_len = 8\n",
    "bat_sz = 4\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make it my net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "modely = nn.Sequential(\n",
    "    nn.Linear(ex_len,n_fl),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_fl,n_ml), # will give 10 outputs (probs)\n",
    "    # nn.Dropout(0.0),\n",
    "    nn.Linear(n_ml,voc_sz),\n",
    "    # nn.Softmax(),\n",
    ")\n",
    "\n",
    "ix = (torch.randperm(len(xtrain)-bat_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    k,d=len(xtest),len(xtest)\n",
    "    for n in range(len(xtest)):\n",
    "        omt = ((modely(xtest[n].float()))).tolist()\n",
    "        # print(omt.index(max(omt)),ytest[n].item())\n",
    "\n",
    "        k -= (omt.index(max(omt))==ytest[n].item())\n",
    "    accu = (((d-k)/d) *100)\n",
    "    return(\"Accuracy = \" + str(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(modely.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 21.29587902458565\n",
      "Accuracy = 21.549997496369734\n",
      "Accuracy = 22.110810675479446\n",
      "Accuracy = 22.08890391067047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(t,Y_train\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# print(loss.item())\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\soumi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soumi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    ahead = 0\n",
    "    for i in ix[ahead :ahead + 100000]:\n",
    "        xt = xtrain[i:i+bat_sz]\n",
    "        yt = ytrain[i:i+bat_sz]\n",
    "        X_train = xt.view([bat_sz,ex_len]) #[4,784]\n",
    "        Y_train = F.one_hot(yt,voc_sz).float()\n",
    "        # print(Y_train.shape)\n",
    "        # break\n",
    "        t = modely(X_train.float())\n",
    "        loss = F.cross_entropy(t,Y_train.float())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(loss.item())\n",
    "    # break\n",
    "    print(accuracy())\n",
    "    if(e == int(0.8*epoch)):\n",
    "        optimizer = torch.optim.SGD(modely.parameters(), lr=0.0001)\n",
    "    ahead +=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA\n",
    "DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "@torch.no_grad()\n",
    "def self_attention(x):  \n",
    "\n",
    "    B,T,C = x.shape\n",
    "    # B = 4  # block size\n",
    "    # T = 8  # token (number of tokens in one example)\n",
    "    # C = 32 # embedding space vector of the each one token\n",
    "    # x = torch.randn(B,T,C) # example sequence\n",
    "    # x -> this is how our one batch should look like here \n",
    "\n",
    "\n",
    "    # head_sz = 32# this is the splits we do of each token seqence to parellelize the sequence gen\n",
    "\n",
    "    key = nn.Linear(C,head_sz,bias=False) # this is to make to complexities truly match  \n",
    "    query =  nn.Linear(C,head_sz,bias=False) \n",
    "    value =  nn.Linear(C,head_sz,bias=False) \n",
    "    k = key(x)  # [B,T,head_sz]\n",
    "    q = query(x)  # [B,T,head_sz]\n",
    "    # this is -- x @ key --  then \n",
    "    # upcoming wei is the main token to token connection matrix\n",
    "    wei = q @ k.transpose(-2,-1)\n",
    "    # wei[0]  # TxT\n",
    "\n",
    "    tril = torch.tril(torch.ones(T, T))  # makes a lower triangular matrix of TxT\n",
    "    wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "    v = value(x)\n",
    "    out = wei @ v\n",
    "    # out[0].shape\n",
    "    return out\n",
    "\n",
    "# basically this is what makes up the new and improved input to the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yt)==len(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# making the mlp for taking the input\n",
    "Model = nn.Sequential(\n",
    "    nn.Linear(head_sz,n_fl),\n",
    "    nn.Linear(n_fl,n_ml),\n",
    "    nn.Linear(n_ml,n_ol),\n",
    "    # nn.Dropout(),\n",
    "    nn.Linear(n_ol,n_octms),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first apply the embedding space to our data\n",
    "\n",
    "embeder  = nn.Embedding(voc_sz,C) \n",
    "optimizer = torch.optim.SGD(Model.parameters(), lr=l_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0494,  0.1003, -2.4102, -0.0692,  0.1053, -0.3065, -0.5829, -0.3122,\n",
      "         0.0683,  0.8160,  0.9703, -2.0326, -1.4490, -0.5732, -1.5804, -1.7114,\n",
      "        -1.0672, -0.2550, -1.0839, -1.3513,  0.7072, -0.4863, -0.4208,  0.2644,\n",
      "         0.3307, -1.0344, -0.1496,  0.3963,  0.7138,  0.7783, -0.7913,  1.0514])\n",
      "tensor([-0.5190,  0.8751, -2.9042,  0.2317,  1.0372, -0.3224, -2.2645, -0.0314,\n",
      "        -0.3553,  0.2634,  0.9047, -2.5852, -1.1306, -1.5207, -1.0926, -1.9229,\n",
      "        -1.4651, -0.1304, -0.5489, -1.6495,  1.0376, -0.4842, -0.1917,  0.7149,\n",
      "         0.4716, -2.2820, -0.4293,  0.6612,  0.2918,  0.8176,  0.0039,  1.1029])\n"
     ]
    }
   ],
   "source": [
    "# a.shape  : [1597676, 8, 32]\n",
    "# now we make the things on a row\n",
    "examples = (Xt.shape)[0]\n",
    "n_iters = 200\n",
    "\n",
    "for i in range(n_iters):\n",
    "    t = random.randint(0,examples-B)\n",
    "    ex = Xt[t:t+B]  # getting training examples\n",
    "    ey = Yt[t:t+B]\n",
    "    with torch.no_grad():\n",
    "        ex = embeder(ex)\n",
    "    slfa_ex = self_attention(ex)  # this would return [B,T,head_sz]  (head_sz : C)here\n",
    "    # we get the attention values to add to the existing ex\n",
    "    print(ex[0][0])\n",
    "    ex += slfa_ex\n",
    "    print(ex[0][0])\n",
    "    break\n",
    "    # final input made \n",
    "\n",
    "    logits = Model(ex)  # this is [4,8,77] i.e in a batch of 4 every token ke baad wale token ka prediction is available\n",
    "    # print(logits[0],ey[0])\n",
    "    loss = F.cross_entropy(logits, ey)  # inputs and outputs in batches\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # if n_iters == 20:\n",
    "    #     break\n",
    "\n",
    "\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshaper(nn.Module):\n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "        super(Reshaper, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "blk_sz = 8\n",
    "emb_sz = 16\n",
    "n_fl = 200\n",
    "voc_sz = len(vocab)\n",
    "inp_sz = 8\n",
    "Model = nn.Sequential(\n",
    "    nn.Embedding(voc_sz,emb_sz),     #  Gets the input ([10000,8,16])\n",
    "    Reshaper(2),  # becomes [10000,4,32]  # [inp_sz,blk_sz//2,emb_sz*2]\n",
    "    nn.Linear(emb_sz*2,n_fl),       # takes [32,200]  becomes [10000,4,200]\n",
    "    nn.Tanh(),\n",
    "    Reshaper(2),  # becomes [10000,2,400]   # [inp_sz,blk_sz//4,n_fl*2]\n",
    "    nn.Linear(n_fl*2,n_fl),    # becomes  [10000,2,200]\n",
    "    nn.Tanh(),\n",
    "    Reshaper(2),  # becomes [10000,400]  # [inp_sz,n_fl*2]\n",
    "    nn.Linear(n_fl*2,voc_sz),    # become [400,voc_sz]\n",
    "    nn.Softmax(),\n",
    ")\n",
    "\n",
    "\n",
    "for x in range(10):\n",
    "    for x in range(20000):\n",
    "        ix = torch.randint(0, X_data.shape[0], (8,))\n",
    "        Xb, Yb = X_data[ix], Y_data[ix] # batch X,Y\n",
    "        # print(Xb)\n",
    "        # break\n",
    "        logits = Model(Xb)\n",
    "        #print(logits , Yb.squeeze(dim = 1))\n",
    "        loss = F.cross_entropy(logits, Yb.squeeze(dim = 1))\n",
    "        Model.zero_grad()\n",
    "        optimizer = torch.optim.SGD(Model.parameters(), lr=0.01)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "f = open('Datasets/Text/RT.txt','r',encoding=\"utf8\")\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
